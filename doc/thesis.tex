%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% This example document / template has been created for      %%%%%
%%%%% use by the students of the MSc in Statistics of Imperial  	 %%%%%
%%%%% College London by Paul Ginzberg on 04 Aug 2014		 %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Set documentclass options.
\documentclass[11pt,a4,singlespacing,titlepagenumber=on]{scrreprt}
%%%obsolete starting points. (ignore)
%\documentclass[a4paper,12pt,singlespace,report]{memoir}
%\documentclass[a4,11pt,singlespace,MSc]{icldt}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% Load required packages with options %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[latin1]{inputenc} % May or may not be needed.
\usepackage[UKenglish]{babel}

%%% American Mathematical Society packages
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsbsy}
%\usepackage{bm} % Possibly a better alternative to amsbsy for making bold typeface math.

%%% Graphics packages
\usepackage{graphicx}
%\graphicspath{{figures/}} % Useful if you have lots of images and want to keep thinks tidy by having a subfolder for images
%\usepackage{epstopdf} % If you produce your graphs as .eps files but then want to compile straight to PDF (e.g. because you are using TeXworks.) you may want to use this option. A better alternative of course would be to save all your graphs as *.pdf files from the start. Note that if you are compiling to pdf through PS/DVI then all your figures should be *.eps files and the epstopdf package should not be used.
%\usepackage{tikz} %For creating vector-graphics diagrams etc directly in LaTeX (takes some time to learn)
\usepackage[absolute]{textpos} % Used to position the Imperial College logo. You can comment this line and the next line out if you don't use the logo.
%\setlength{\TPHorizModule}{\paperwidth}\setlength{\TPVertModule}{\paperheight}
%\setlength{\TPHorizModule}{1cm}\setlength{\TPVertModule}{1cm}


%%% Referencing and cross-referencing
%\usepackage{color}
%\usepackage[colorlinks=false,linkcolor=red,urlcolor=cyan,citecolor=blue,breaklinks,plainpages=false,pdfpagelabels]{hyperref} % To make the hyperlinked cross-referencing visible.
\usepackage[colorlinks=false,pdfborder={0 0 0},plainpages=false,pdfpagelabels]{hyperref} % If you click on an item in the table of contents or a referenced equation/figure number, the PDF will go to the desired page. Neat isn't it?
% \usepackage[round,authoryear,sort]{natbib} % Enable bibtex-based bibliography generation
\usepackage{cite}
%\usepackage[square,numbers,sort&compress]{natbib} % If you want numbered referencing instead of author-year style.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Create or control Macros   %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\setcounter{secnumdepth}{3} %If you want subsubsections to be numbered
\numberwithin{equation}{chapter} % Reset equation numbers after each chapter.

%%% Theorem environments
\newtheorem{theorem}{Theorem}%[chapter]
\newtheorem{proposition}[theorem]{Proposition}%[chapter]
\newtheorem{definition}[theorem]{Definition}%[chapter]
\newtheorem{lemma}[theorem]{Lemma}%[chapter]
\newtheorem{corollary}[theorem]{Corollary}%[chapter]
%
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}%[chapter]
\newtheorem{example}[theorem]{Example}%[chapter]

%%% Potentially useful style changes:
%\renewcommand{\titlefont}{\normalcolor \normalfont \bfseries} %Change the title font from sans-serif to serif (the same font used for the rest of the document).
%\renewcommand*{\labelitemi}{$\bullet$} %Bullet points in the itemize environment.
%\renewcommand*{\tilde}{\widetilde} % Wider tildes
%\renewcommand*{\bar}{\overline} % Wider conjugate bars

%%% Examples of commands/macros that could be useful:
%\newcommand{\expectation}[1]{\mathbb{E}\left[ #1 \right]}
%\newcommand*{\setR}{{\mathbb R}}
%\newcommand{\commentify}[1]{} %Gives you an alternative way (other than %) to comment things out.
%%% These commands make it faster to get the correct roman font in equations: (similar to \exp, \cos, \sin etc). Alternatively yuo can always use e.g. $\mathrm{Var}$
%\DeclareMathOperator{\bigo}{O}
%\DeclareMathOperator{\littleo}{o}
%\DeclareMathOperator{\var}{Var}
%\DeclareMathOperator{\cov}{Cov}
%\DeclareMathOperator{\trace}{trace}
%\DeclareMathOperator{\sign}{sgn}
%\DeclareMathOperator{\rank}{rank}
%\DeclareMathOperator{\vecrm}{vec} % The \vec command already exists.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Define how to create the title page  %%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\makeatletter
\newcommand*{\supervisor}[1]{\gdef\@supervisor{#1}}
\newcommand*{\CID}[1]{\gdef\@CID{#1}}
\newcommand*{\logoimg}[1]{\gdef\@logoimg{#1}}
\renewcommand{\maketitle}{
\begin{titlepage}
\ifdefined\@logoimg
\begin{textblock*}{8cm}(1.75cm,1.75cm)
\includegraphics[width=70mm]{\@logoimg}
\end{textblock*}
\vspace*{1cm}
\else
%\vspace*{0cm}
\fi
\begin{center}
\vspace*{\stretch{0.1}}
Imperial College London\\
Derpartment of Mathematics\par
\vspace*{\stretch{1}} % This inserts vertical space and allows you to specify a relative size for the vertical spaces.
{\titlefont\Huge \@title\par} % If your title is long, you may wish to use \huge instead of \Huge.
\vspace*{\stretch{2}}
{\Large \@author \par}
\vspace*{1em}
{\large CID: \@CID \par}
\vspace*{\stretch{0.5}}
{\large Supervised by \@supervisor \par}
\vspace*{\stretch{3}}
{\Large \@date \par}
\vspace*{\stretch{1}}
{\large Submitted in partial fulfilment of the requirements for the
MSc in Statistics of Imperial College London}
\vspace*{\stretch{0.1}}
\end{center}%
\end{titlepage}%
}
\makeatother

%%% And the plagiarism declaration
\newcommand*{\declaration}{%
\vspace*{0.3\textheight}
The work contained in this thesis is my own work unless
otherwise stated.\\
\vspace*{0.1\textheight}\\
\hspace*{0.25\textwidth}Signed: \hspace{0.25\textwidth} Date:
\clearpage}

%%% And the abstract page
\renewenvironment{abstract}%
{\chapter*{Abstract}\thispagestyle{plain}}%
{\clearpage}
%%% And why not change the quote environment
\newenvironment{myquote}%
{\begin{quote}{\Large{}``}}%
{{\Large{}''}\end{quote}}




%%% Actual words used in the title page
\title{Adaptive learning for the restless bandit problem, with an application to online advertising}
\author{Alex Gillis}
\CID{00881910}
\supervisor{Supervisor Dr. Christoforos Anagnostopoulos}
\date{\today}
\logoimg{Imperial__4_colour_process.jpg} %The filename for the logo, if you want one. The logo file must be in the same directory as your *.tex file.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% End of preamble and start of document %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}


\maketitle %Generates the Title Page

\declaration %Insert plagiarism statement

\begin{abstract}

Our work formulates the realtime bidding problem as a multi-armed bandit. In particular the work focuses on the relationships between dependent binomial variables, with the ability to model a K-stage binomial proces and offering improved decision rules for MAB style problems.

So far we have assumed each 'lever' to be a website. Though models could just as well apply to a particular placement or creative.

\end{abstract}

\newpage
\chapter*{Acknowledgements}
Thank you supervisor/friends/family/pet.
\begin{myquote}
Include an acknowledgement.
\end{myquote}
\newpage

% Automatically create a table of contents
\renewcommand{\contentsname}{Table of Contents}
\tableofcontents
\newpage

% Figure and table lists if you want them.
%\cleardoublepage
%\phantomsection
%\listoffigures 
%\addcontentsline{toc}{chapter}{\listfigurename}
%\newpage
%\phantomsection
%\listoftables  
%\addcontentsline{toc}{chapter}{\listtablename}
%\newpage



\chapter{Introduction}

Our research seeks to formulate the real time bidding (RTB) problem as a multi-armed bandit (MAB). There are no papers addressing this directly, we therefore study RTB and MAB research separately. There are a several dimensions - estimating CTR rate for different placements, estimating acquisition rate and optimizing bid level over course of campaign.

\section{Real Time Bidding}

TODO - give quick background on RTB, CTR, CVR before launching in to lit review.

Richardson et al. ~\cite{Richardson:2007:PCE:1242572.1242643} discuss the benefits and limitations of a priori CTR prediction. Chen et al ~\cite{Chen:2011:RBA:2020408.2020604} and Ghosh et al. (Adaptive Bidding) ~\cite{Ghosh:2009:ABD:1526709.1526744} develop algorithms which optimize bid levels online. While Chen et al ~\cite{Chen:2011:RBA:2020408.2020604} focus on dynamic bid adjustment strategies to optimally meet campaign constraints, Ghosh et al. ~\cite{raey} look to 'learn' the bid distribution acknowledging the E-E tradeoff, though not explicitly formulating as a MAB problem.

Li et al. ~\cite{Li:2010:CAP:1772690.1772758} explore the contextual multiarmed bandit problem as applied to news article recommendation, while Yue and Pandey use knowledge about the item and the arm inter-relatedness to speed convergence of the bandit arms.

No papers were found formulating the RTB as a MAB explicitly. Also, many practical aspects of the RTB problem have been overlooked - such as predicting acquisition rate, optimal explore-exploit tradeoffs for ad placement given time/budget limited campaigns. There also appears room to improve the synthesis of online and offline optimisation by applying Bayesian principles.

\section{Objective}
Our objective is to develop a model which minimizes the cost per acquisition over the course of a campaign, whilst meeting budget constraints. We propose a number of innovations:
\begin{enumerate}
	\item A model to estimate CPA (as well as CTR).
	\item To elegantly address the tradeoff between offline and online estimation (discussed by Richardson) by offline generation of Bayesian prior for CTR.
	\item To model the E-E tradeoff as a MAB problem, putting a value on exploration for long and short campaigns.
	\item To incorporate the time and budget limitations in exploration of the MAB problem.
\end{enumerate}


\chapter{RTB Data}

We have 3 vector valued random variables. The ith index represents the values for site domain i for a particular lineitem. In the context on a MAB problem, each index represents a lever. The variables are defined as:
\begin{itemize}
	\item \textbf{n} - number of impressions served over given time period
	\item \textbf{c} - number of clicks on impressions over same time period 
	\item \textbf{a} - number of acquisitions over same time period
\end{itemize}

\section{Exploratory analysis}

\includegraphics{CTRvsCVR}

TODO - bring in relevant sections of EDA document here.

\chapter{Models}

\section{Definitions} 

We define 7 models to fit the data, ultimately intending to model acquisitions.

We start with simple binomial model (m1), then look to beta binomial (m3) to deal with over-dispersion. We then bring in intermediate clicks, hoping these can offer more information on the likelihood of future acqusitions. We find that clicks provide no information on acquisitions unless there is a dependency between p and q. We consider both a cluster model as well as a correlation model to model dependency between p and q.

\subsection{Model 1 - Single Binomial} 
All levers are assumed to have the same payoff function. A useful baseline model. We assume a uniform Beta prior so that the MAP estimate of r is equivlanet to the MLE and is also an unbiased estimator. We gain as much information about lever 1 by looking at lever 2 as by looking at lever 1.

  \begin{align}
	a_i \sim Bin(r,n_i) \\
	\pi(r) = Beta(1,1)
  \end{align}

\subsection{Model 2 - Independent Binomials} 
We assume that each lever has a completely independent payoff function. There is no information to be gained about lever 1 by looking at lever 2.

  \begin{align}
	a_i \sim Bin(r_i,n_i) \\
	\pi(r_i) = Beta(1,1)
  \end{align}

\subsection{Model 3 - Beta-binomial} 
Useful if data is over dispersed for a binomial model. Here we assume a hierarchical model where the conversion rate for each lever is sampled from a Beta distribution with common $\alpha$, $\beta$ parameters across all levers. In a sense, this model lies somewhere between model 1 and model 2, in that data about lever 1 gives us some information about lever 2 via inference of $\alpha$, $\beta$ parameters. 

  \begin{align}
	a_i \sim Bin(r_i,n_i) \\
	r_i \sim Beta(\alpha,\beta) \\
	\pi(\alpha) = Unif(0,10000) \\
	\pi(\beta) = Unif(0,10000)
  \end{align}

\subsection{Model 4 - Beta-binomial plus clicks} 
Since acquisitions are rare events, we try to gain more information about a lever be modelling the click-through-rate separately from the conversions. Similarly to model 3, CTR and CVR have a hierarchical model.

  \begin{align}
	a_i \sim Bin(q_i,c_i) \\
	c_i \sim Bin(p_i,n_i) \\
	q_i \sim Beta(\alpha_q,\beta_q) \\
	p_i \sim Beta(\alpha_p,\beta_p) \\
	\pi(\alpha_q) = Unif(0,10000) \\
	\pi(\beta_q) = Unif(0,10000) \\
	\pi(\alpha_p) = Unif(0,10000) \\
	\pi(\beta_p) = Unif(0,10000) 
  \end{align}

When p and q are independent of each other, $a_i$ has distribution $Bin(p_iq_i,n)$ and $p_iq_i$ has distribution $Beta(a_i,n_i - a_i)$ (see 'Do Clicks Matter' for proof). This makes it equivalent to model 3 with different priors. The intuition here is that clicks are not informative - as much as they increase our belief in click through rate, they decrease our belief in the acquisition rate.

\subsection{Model 5 - 2 cluster Binomial} 

Useful for comparison against model 6.

\subsection{Model 6 - 2 cluster Beta-binomial }

Model 4 assumed that CTR and CVR are independent. Experience suggests that levers tend to perform well on both CTR and CVR or poorly on both. In other words, there tend to be 'clusters' of good and bad levers. Model 5 attempts to capture this with a beta-binomial mixture model. We use priors to bias one cluster towards being the poor performer, this helps avoid 'index switching' complications with model fitting. 
 
 \begin{align}
	a_i \sim Bin(q_i,c_i) \\
	c_i \sim Bin(p_i,n_i) \\
	k \sim Bern(h) 
\end{align}
	\[ 
	q_i \sim 
  	\begin{cases}
		Beta(\alpha_{q0},\beta_{q0}) & \quad \text{if k is 0}\\
		Beta(\alpha_{q1},\beta_{q1}) & \quad \text{if k is 1}
	\end{cases}
	\]
	\[
	p_i \sim 
  	\begin{cases}
		Beta(\alpha_{p0},\beta_{p0}) & \quad \text{if k is 0}\\
		Beta(\alpha_{p1},\beta_{p1}) & \quad \text{if k is 1}\\
	\end{cases}
	\]

 \begin{align}
	\pi(h) = Unif(0,1) \\
	\pi(\alpha_{q0}) = Gamma(1,2) \\
	\pi(\beta_{q0}) = Gamma(1,2) \\
	\pi(\alpha_{p0}) = Gamma(1,2) \\
	\pi(\beta_{p0}) = Gamma(1,2) \\
	\pi(\alpha_{q1}) = Unif(0,10000) \\
	\pi(\beta_{q1}) = Unif(0,10000) \\
	\pi(\alpha_{p1}) = Unif(0,10000) \\
	\pi(\beta_{p1}) = Unif(0,10000)
\end{align}

\includegraphics{GraphMod.jpg}

\subsection{Model 7 - Correlated Beta-binomial }

'Bacon With Your Eggs? Applications of a New Bivariate Beta-Binomial Distribution'
'The Use of a Correlated Binomial Model for the Analysis of Certain Toxicological Experiments'
Lee - 'Properties and Applications of the Sarmanov Family of Bivariate Distributions'

As an alternative to model 6, where rate prediction will change abruptly between clusters, we look for a model where q can vary smoothly as a function of p.

We use a relatively unused binomial correlation model, discussed in 'Bacon With Your Eggs? Applications of a New Bivariate Beta-Binomial Distribution'. 

\section{Synthetic data plots}

\includegraphics[scale=0.7]{SynthData}

\section{Testing for fit}

We take the approach of starting with a simple model and run further tests to decide whether there is evidence to expand the model. 

Where possible we prefer tests with strong theoretical backing, e.g. LR. As model complexity rises it becomes harder to rely on these. We use information criteria and posterior validation.

\subsection{test 1} A frequentist style goodness of fit test designed to assess whether data can be well explained with simple binomial model 1. We use the binomial approximation to the Normal and perform pearson chisq on the residuals. This only holds for large n, so we group the small n values to test whether (together) they share the same mean.

Benefits are that we can have a composite alternative hypothesis, downside is the approximation used. Could be improved by Yates' corerction. TODO - approximation fails for p as rate is very small.

'Goodness-of-Fit Issues in Toxicological Experiments Involving Litters of Varying Size'

\subsection{test 2}

Assuming we reject model 1, we try LR test to see if data is better modelled using beta-binomial. Testing null hypothesis against all others is difficult. LR test has the benefit of being 'most powerful' for comparing 2 point hypotheses, so is a good choice assuming we are happy using ML point hypotheses. If we were willing to specify priors on parameters, we could turn this in to a Bayes factor test.

TODO - compare ratio value to alpha, or do chisq on deviances?

Although this test can show whether Model 3 is better, it can not show whether it is 'good'. This paper has excellent review of goodness-of-fit tests for beta-binomial model:
'Bootstrap goodness-of-fit test for the beta-binomial model'

We use both VGAM and Stan package to get ML estimates for beta-binomial.

\subsection{test 3}

LR test Model 1 vs Model 5 - 1 cluster binomial vs 2 cluster binomial. 

This is a cheap - 'are there clusters between median values' test. 

\subsection{test 4}

Assuming we reject null in test 2, due to some form of over dispersion, we check if can extend this to 2 cluster model with another LR test. Obviously we could keep testing for additional clusters. A more general approach could be to define prior penalizing large number of clusters and use VB or Gibbs to find posterior.

This is a cheap - 'are there clusters between median values' test. 


\subsection{test 5}

Use AIC and BIC for comparison between Model 6 and Model 7. These models aren't nested.

\subsection{Posterior predictive checks}

Similar to the goodness of fit checks, these let us compare our model against observed data. First can use the generating functions to visuallly compare.

\subsubsection{Bayesian p-values}

If we can choose a scalar test statistic, we can produce a p value of realized statistic versus model theoretic probably of the statistic being more extreme than that. Not an obvious candidate.

\subsubsection{Chi-square GoF}

Possibly very useful, need to be careful about the validity of the approximation given some small n.

\subsection{Correlation measures}

% latex table generated in R 3.0.2 by xtable 1.7-3 package
% Wed Jul 23 18:50:50 2014
\begin{table}[ht]
\centering
\begin{tabular}{rlrrrrrrrr}
  \hline
 & g & Cor & IWC & logIWCc & RankCor & items & imps & glmcoef & glmsignif \\ 
  \hline
1 & m1 & -0.03 & -0.00 & -0.01 & 0.01 & 1681 & 12946112 & 0.12 & 0.00 \\ 
  2 & m\_1023111 & -0.07 & 0.03 & 0.10 & 0.20 & 191 & 23228621 & -0.03 & 0.19 \\ 
  3 & m3 & -0.07 & -0.02 & -0.13 & -0.09 & 364 & 13079687 & -0.10 & 0.01 \\ 
  4 & m5 & 0.14 & 0.05 & 0.37 & 0.27 & 324 & 12815129 & 0.76 & 0.00 \\ 
  5 & m6 & 0.03 & 0.04 & 0.23 & 0.05 & 319 & 12743695 & 0.22 & 0.00 \\ 
   \hline
\end{tabular}
\end{table}


\subsection{Diagnostic tests}

TODO - cluster test fails to identify model 5.
TODO - vgam library has problems fitting non betabinom cluster data.
% latex table generated in R 3.0.2 by xtable 1.7-3 package
% Wed Jul 23 18:51:42 2014
\begin{table}[ht]
\centering
\begin{tabular}{rlrrrl}
  \hline
 & g & test1\_BinomGoF & test2\_BinomVsBetaBinom & test3\_2ClustBinom & test4\_2ClustBetaBinom \\ 
  \hline
1 & m1 & 1.00 & 1.00 & 1.00 & Error \\ 
  2 & m\_1023111 & 0.00 & 0.00 & 1.00 & Error \\ 
  3 & m3 & 0.00 & 0.00 & 1.00 & Error \\ 
  4 & m5 & 0.00 & 0.00 & 1.00 & Error \\ 
  5 & m6 & 0.00 & 0.00 & 0.00 & 0 \\ 
   \hline
\end{tabular}
\end{table}

\section{Summary}

\begin{itemize}
	\item Need to use log values of p.
	\item Clustering based on $a_i$ creates false correlations.
	\item Clustering based on c does not create false correlations.
	\item Using c greater than 0, correlations do not show up. But become stronger for c gt 1, c gt 2. This is because it reduces the noise of all the small-n zero and one values.
	\item Filter based on realized CTR is no use as it does not help with (3). Needs a correlation measure which properly accounts for likelihood weight of response - closest thing is glm.
	\item Binomial GLM does the best job of (4), though does not consider error in dependent variable.
\end{itemize}


\chapter{ Multi-armed Bandits}

\section{ Markov Decision Processes }

A markov decision process is a formalism for describing sequential decision problems in discrete time. An MDP consists of a 4-tuple of states $\mathcal{S}$, actions $\mathcal{A}$, state transition probabilities under each action $P_a(s,s')$, and reward distributions for the each transition $\mathcal{R}_a(s,s')$. The goal is to seek policy $\pi$ which maximizes the rewards over the lifetime of the process. For each time $t$:

\begin{align}
a_t &\in \mathcal{A} \\
s_t &\in \mathcal{S} \\
r_t &\sim \mathcal{R}_{a_t}(s_t,s_{t+1})
\end{align}

\includegraphics[scale=0.5]{MDP_example.png}

\subsection{ Dynamic programming and optimal control }

For a finite number of time steps, MDPs have optimal policies defined by the Bellman equation. The dynamic programming equation assigns a value to each action equal to the expected immediate reward plus the expected future rewards $V$ of the state transitioned to, under an optimal policy.

The optimal policy is then to greedily choose action with highest value.

\begin{align}
	V(s_0) &= \max_{a \in \mathcal{A}} E_{s'}[\mathcal{R}_a(s_0,s')] \\
	V(s_t) &= \max_{a \in \mathcal{A}} E_{s'}[\mathcal{R}_a(s_t,s') + V(s')]
\end{align}

\subsection{ Q-learning }

When the rewards are unknown, this becomes the reinforcement learning problem. Q-learning TODO Watkins and Dayan 1992 is a popular framework for iteratively estimating the state-action value function defined by the Bellman equation. Here $Q_t$ is our estimate of the value function, and $\alpha$ is a 'learning rate' parameter:

\begin{align}
Q_{t+1}(s_t,a_t) = Q_t(s_t,a_t) + \alpha\{r_{t+1} + \max_a Q_t(s_{t+1},a) -Q_t(s_t,a_t)\}
\end{align}

Q-learning is 'model free' in that it makes no explicit assumptions about the relationship between actions, states and rewards. It can also be used in conjunction with a function approximation such as linear function TODO, or neural-net TODO deepmind. This can make large models more tractable at the cost of losing convergence guarantees.

The algorithm also requires an explicit policy for exploring sub-optimal actions, suc as $\epsilon$-greedy (see section ~\nameref{sec:heuristics}).

\section{ Multi-armed bandits and regret}

A multi-armed bandit is a single state MDP with unknown rewards. It has K actions, or 'arms'. The name comes from the analogy with choosing how to bet on a choice of K slot machines, which are sometimes known as one-armed bandits.

\begin{align}
&\mathcal{A} = \{1,...,K\} \\
&a \in \mathcal{A} \\
&r_t \sim \mathcal{R}_{a_t} 
\end{align}

Typically bandits policies are assessed by how well they minimize \textit{regret}. The regret of policy $\pi$ is defined as the expected difference from the \textit{oracle} policy of always choosing arm with maximum expected reward:

\begin{align}
&\mu_a = E[r|a] \\
&\mu^{*} = \max_{a \in \mathcal{A}} \mu_a \\
\operatorname{regret}(\pi) &= \sum_{t=1}^T (\mu^{*} - \mu_{\pi(t)})
\end{align}

\section{ Bayes Adaptive policy }

The graphs below provide a demonstration of this policy. In each graph, we see which arm choice is optimal as a function of rounds remaining. Note the high expectation arm is preferred for short campaigns the moves toward the higher variance arms as the time horizon increases. The 6 panes show one possible scenario play out as arms are chosen and posteriors are updated.

\includegraphics[scale=0.7]{BARLillustration.pdf}

This solution has a number of particularly attractive characteristics over probability matching strategies such as Thompson sampling and UCB:
\begin{itemize}
	\item We see that for small numbers of rounds, the algorithm purely exploits the high expectation arms, since there are too few rounds for the high variance arms to produce enough positive results to change our decisions. This is very relevant for our domain where we may have millions of arms (sites x placements x creatives x userType) but only a few 100,000 rounds and expectation of only a few 100 acquisitions. This addresses the issues we saw in previous section where the click only model would benefit from early exploitation of non-optimal arms. Thompson samply and UCB would typically over explore in these situations too.
	\item We can still integrate our prior information.
	\item If arms have some common dependency, such as using the same creative it understands there is greater exploration value.
	\item Policy responds to changes how we value events, underlying model, time horizons etc.
	\item The approach is orthogonal to the underlying model, so that we can develop the underlying model without needing to update the decision algorithm.
\end{itemize}

The limiting factor for this approach is that the algorithm has exponential order cost and so can be computed exactly only for small games. Various sampling based approximations exist \texttt{http://www.gatsby.ucl.ac.uk/~dayan/papers/guez\_jair2013.pdf}. 


\section{ Heuristics } \label{sec:heuristics}

There are a variety of heuristic approaches, these tend to be simple to calculate and often provide tunable parameters though have no theoretic guarantees. See TODO steven L scott for a more extensive overview.

\begin{itemize}
	\item $\epsilon$-greedy - choose arm with highest expectation $\epsilon$ times, then choose a sub-optimal arm for exploration. The $\epsilon$ value can decrease over the life of the game.
	\item POKER - TODO 'Multi-armed Bandit Algorithms and Empirical Evaluation' attempts to assign an exploration value to each arm. It makes a number of assumptions to estimate the probability the arm offers an improvement and the size of that improvement. The exploration value is then the probability weighted improvement multiplied by the number of rounds remaining to exploit it. Although it aims to capture the qualities we would expect in a good decision policy, in our view it does this artificially through the assumptions it makes. By comparison, these qualities arise quite naturally from the dynamic programming solution.
\end{itemize}

\section{ Thompson Sampling }

Thompson sampling is a non deterministic strategy. The policy chooses arm $j$ with frequency $w_j$, which is the posterior probability that it is the optimal arm.

\begin{align}
w_j = P(\mu_j > \max(\mu_1, ..., \mu_k))
\end{align}

Agrawal and Goyal ~\cite{DBLP:journals/corr/abs-1111-1797} prove asymptotic optimality for the binomial case, as well as demonstrate finite horizon performance improvement over KL-UCB and UCB using simulation studies.

\section{ Gittins Index }

Gittins ~\cite{gittins1979bandit} was able to show that the MAB problem with geometrically discounted regret could be treated as an index problem, meaning that the optimal decision rule is to select arm with highest index. That index could be calculated using only information about the arm. The intuition here is that since playing one arm does not affect the rewards of other arms, a value can be computed using only information about that arm. The value is the expected rate of reward under an optimal stopping rule.

\begin{align}
v(x) = \sup_{\tau > 0} E\left[ \sum_{t=0}^{\tau-1} a^t \mathcal{R}(s_t) | s_0 = s  \middle/  E\left[ \sum_{t=0}^{\tau-1} a^t | s_0 = s \right]  \right]
\end{align}

Although the Gittins index is not optimal in the finite horizon case (the value of alternate arms changes as a result of the loss of 1 round), it is a very close approximation. Our testing (not presented) confirms this GI and BA strategies achieve near equal regret.

As the calculation involves only 1 arm, the computation has significantly lower order, with the potential to share results between arms. Comparing the the Bayes Adaptive solution $O((\#\mathcal{A} \#\mathcal{P})^T)$, we instead have:

\begin{enumerate}
	\item $O(\#\mathcal{A} {\#\mathcal{P}}^T)$ - Exact method.
	\item $O(\#\mathcal{A} T^{\#\mathcal{P}})$ - Calibration method.
\end{enumerate}

\subsection{ Exact method }

\subsection{ Calibration method }

There are several methods looking for more computationally efficient calculation, the most popular being the calibration method where a grid of candidate GI values is used to obviate the need to recurse. See Nino-Mora ~\cite{nino2011computing} for an overview. The cost for these methods is still of order at least $rounds^3$ (depending on matrix multiplication algorithm used) and so will not scale to say a million rounds.

The calculation of this index typically involves choosing a number of rounds to look ahead, the less the discount factor, the more rounds are required to get a close approximation. Then calculate the value at round N and use backwards induction to calculate the value for round 0.

We present results comparing GI to TS strategies with arms = 10, rounds = 50. We use the exact method described in Gittins 1979. Note that since this is a finite horizon game, there is no loss of accuracy associated with calculating a finite number of rounds.

We demonstrate in the appendix that it is simple to adapt the GI for binomial bandits to the both model as it only requires taking expectations.

\subsection{ Approximations }

Noting that GI is a function of only 5 variables, we consider approximations. Brezzi and Lai ~\cite{brezzi2002optimal} provide a closed form (i.e. constant order) solution to the infinite horizon problem using a Normal approximation by relating the discrete time problem to a continuous time Wiener process.  The same paper references Lai ~\cite{lai1987adaptive} for asymptotically optimal solutions for the finite horizon, undiscounted problem for single parameter exponential family distributions. Burnetas and Katehakis ~\cite{burnetas1996optimal} extend this to multiparameter univariate distributions.

The multiparameter solution would fit our both model well and provide opportunity to extend the funnel to include additional intermediate steps. Unfortunately, as earlier noted, our testing finds these approaches do not work well with the arms, rounds and rates we work with.



\section{ Lai and Robbins lower bound }

\begin{align}
\operatorname{regret}(T)  \leq  \frac{\ln(n)}{D_{KL}(\mu_j||\mu^{*})} % as n \to \inf
\end{align}

Lai and Robbins ~\cite{lai1985asymptotically} proved that a class of decision algorithms meeting certain regret conditions were asymptotically optimal learners. This insight is the basis for the deterministic probability matching, UCB algorithms. 


We note that the optimal asymptotic regret leaves scope for a large constant regret term, as we found in our experiments poor performance over limited horizons.

We conjecture that due to the high number of arms, and low success rate, in practise we do not get to the region where asymptotic behaviour dominates.

The bayes-optimal solutions remain attractive. Although the Bellman equation gives us optimal solution, the computational time has order ${arms}^{rounds}$ and so becomes infeasible for large trials. We discuss the options providing similar results with less computation.

\section{ UCB1 }

There is a large family of index algorithms based on assigning an 'upper confidence bound' value to each arm. In these, we allow the estimated payoff $\hat{\theta}$ of the arm to increase up to a constraint based a maximum KL divergence of the parameter. The $\lambda$ value controlling the constraint will typically vary based on the number of times the arm has been played and the current round.

\begin{align}
u_j = \inf\{ \theta : \theta \geq \hat{\theta}, D_{KL}(\theta||\hat{\theta}) \geq \lambda\} 
\end{align}

Where $n$ is the current round, $n_j$ is the number of times arm $j$ has been played, UCB1 algorithm assigns the arm an index according to rule:

\begin{align}
u_j = \hat{\theta_j} + \sqrt{ \frac{2 \ln{n}}{n_j}  }
\end{align}

Agrawal and Goyal ~\cite{DBLP:journals/corr/abs-1111-1797}, in their analysis of Thompson sampling demonstrated superior performance from Thompson sampling compared to a variety of modern UCB algorithms for binomial problems.

\chapter{ Results }

\section{Test harness}

A simulation test harness was set up to simulate MAB trials. Custom policies were run in the harness and assessed against a regret function. We define regret as the difference in r between chosen arm and optimal arm (TODO add notation).

\begin{enumerate}
	\item Experimenter defines a bivariate Beta prior distribution parameterized by alpha and beta values for p and q.
	\item This prior is then used as a generative model to sample a set of 'true' p and q values for each arm.
	\item Each strategy is provided with the prior information and must infer the true values of p and q. The test harness calls on the strategy to return an arm number. The test harness then draws a single sample from a distribution using the true p and q values for this arm. The result is a tuple $(acquisition,click,view)$. The strategy is given this result so that it may update it's knowledge. The mean regret for the strategy is also updated.
	\item Step 3 is repeated for a set number of rounds i.e. the time horizon.
	\item Steps 2, 3 and 4 are looped 100 times so that we can calculate mean regret. Standard error for the mean values is estimated using a bootstrap method.
\end{enumerate}





\section{Thompson Sampling}

\begin{description}
	\item[ts clicks] Each arm is initialized with a beta prior using the alpha and beta values for p. Arms are chosen using Thompson sampling based on the probability of each arm having maximum p. The algorithm seeks to optimize clicks only, though regret is still calculated in same way.
	\item[ts both] Each arm is initialized with beta prior for both p and q. Arms are chosen using Thompson sampling based on the probability of each arm having maximum r value (i.e. pq). Since the distribution of pq does not in general have an analytic form, we use sampling. The priors on p and q are updated in the usual way.
	\item[ts acqs] The strategy only looks at acquisitions per view, ignoring clicks. Since the Beta priors on p and q do not directly translate in to a Beta prior on r, we estimate the parameters of a Beta prior for r using the mean and variance of p and q. This new Beta prior is updated in the usual way with acquisitions and views.
\end{description}

\subsection{Validation}

We run a set of validation checks to make sure we are not favouring one strategy in some extrinsic way - such as a more informative prior or more accurate posterior sampling.

\begin{enumerate}
	\item  We give p a point prior at value 1. With this, we expect to see the same performance from ts acqs and ts both. ts clicks should select arms arbitrarily.
	\item  We give q a point prior at value 1. With this, we expect to see the same performance from all 3 strategies as they are all 'learning' click rate.
	\item  We give both p and q flat priors. We expect to see the same performance from ts acqs and ts both as our analysis shows this is a special case, equivalent to modelling acquisitions only.
\end{enumerate}

\subsection{Plots}

\includegraphics[scale=0.35]{P4to5UCB}

\includegraphics[scale=0.7]{P1to6.pdf}

\subsection{Discussion}

\begin{description}
	\item[ts clicks] typically reduces regret very quickly particularly with few arms and high variance on p. But over longer periods loses out to ts acqs and ts both since the best arm does not always have the highest rate of clicks. If there is a high variance on q, it can go very wrong.
	\item[ts acqs] performs well for long trials or where q can take on high values. But typically learns slowly, particularly when q has small values.
	\item[ts both] robust to the pathological cases that see poor performance from ts clicks and ts acqs, and in certain cases produced significantly improved results on both. In all but one instance (discussed next), ts both is at least as good as either algorithm.

\end{description}

We also note that ts clicks initially beats ts both, though this advantage goes away once the number of arms is increased. We hypothesise this is a feature of Thompson sampling rather than the model - by ignoring some of the uncertainty in the true value of r, ts clicks gains an early exploitation advantage.

We also note that as the number of arms goes up, the regret increases. This could be because the 'best arm' becomes further from the average arm given more arms, but possibly a sign of over-exploration.

We consider these are issues concerning the decision algorithm (Thompson sampling) in the next section.

\pagebreak

\section{ Bayes Adaptive }

In the previous section, we demonstrated that a more sophisticated model offered stronger inference and could reduce MAB regret. But we also noted a couple of instances where performance was not as good as it might be. 

With this in mind, we now look at options for decision algorithms. In particular we ask the question - given varying number of arms and campaign length, what is the best policy? 

PAC learning approaches look to limit the number of sub-optimal decisions (TODO add references). But this pure exploration approach does not naturally map to our goal of maximizing value from our campaign budget.

For given priors, it is possible to define a 'Bayes-optimal' policy. A MAB problem is a one state MDP with unknown payoffs. We can turn this in to a many state MDP with known payoff functions. Each state represents one possible value of the posterior. Each transition probability is given it's expected value based on the current posterior. This is called the Bayes Adaptive MDP. Now that we know the action rewards and transition probabilities, the optimal decision policy is provided by the Bellman equation from optimal control theory.

TODO - Reference Bayes Adaptive Reinforcement learning,
Optimal Adaptive Policies for Markov Decision Processes.

\begin{description}
	\item[ba both] Each arm is initialized with beta prior for both p and q. Arm with maximum Q value is always chosen, where multiple arms have same Q value, one is selected at random.
	\item[ba acqs] The strategy only looks at acquisitions per view, ignoring clicks. Since the Beta priors on p and q do not directly translate in to a Beta prior on r, we estimate the parameters of a Beta prior for r using the mean and variance of p and q. This new Beta prior is updated in the usual way with acquisitions and views. The arm selected is the one with maximum Q value.
\end{description}




\subsection{Plots}

Using the same harness, we set up the experiment using a 'bayes-optimal' exploration algorithm. Due to the exponential order cost of this algorithm, we must keep the games to a small number of rounds. 

\includegraphics[scale=0.9]{TSvsBARL.pdf}

We see both Bayes Adaptive strategies improve on their Thompson Sampling equivalents. The strategy combining the improved model as well as the optimal decision policy offers very significant benefits.






\section{Gittins Index}



\subsection{Plots}

\includegraphics[scale=0.7]{GIvsTS.pdf}

\includegraphics[scale=0.7]{GIBoth.pdf}


\chapter{Extensions}

\section{Applicability to other models}

TODO - discuss how above MAB strategies can be applied to mixture models (m5 and m6) and correlation / copula models (m7).

\section{Contextual bandits}

We hope that the models can be further expanded to include a more complex hierarchy, for example, campaign-site-placement. Or could dependencies on for example, fixed effect regressors for creative or user category.

\section{Scaling up}

We may extend the $r$ round, 2-stage (clicks an acquisitions) model to $s$ stages. Using Gittins index, we will have $r^s$ states in the final round, meaning we will have an $r^s$ by L matrix, which we perform $r$ matrix multiplications.

There are several ways to reduce the computational cost due to the number of arms - grouping similar arms (e.g. contextual / factorial bandits ) or reuse of similar GI calcs. But the limiting issue is the cost due to large number of rounds.

Given that we are often working with very small rates, we might calculate GI by grouping the updates and enumerating only the more likely outcomes. For example, with a rate of 0.5\%, we might group by 1000 rounds and consider outcomes 0 to 12 which covers 0.998 of the distribution. 
















\cleardoublepage
\phantomsection
% \addcontentsline{toc}{chapter}{\bibname} % Add an entry for the Bibliography in the Table of Contents
%\pagestyle{headings}
% \bibliographystyle{abbrvnat} % set the bibliography style
% \bibliography{bibtexfile} % generate the bibliography
%\bibliographystyle{abbrvnat} % <- Mistake in earlier version. After the bibliography is created it's too late to change the style.
% Pick a sensible bibliography style. 
% If like above you choose to use author-year style citations, you must choose a natbib-compatible bibliographystyle such as 'plainnat'. Abbrvnat is one of the few built-in styles of this type. % You may find many more bibliography styles (*.bst files) online. You can also choose to write your bibliography manually instead of using BibTeX (This will take you longer, unless you plan to use the content of a BibTeX-generated *.bbl file as your starting point).
% When creating a BibTeX file using e.g. reference manager software, note that the entries may contain additional unwanted fields which you would then need to erase. For example, you probably don't want to include the URL of a journal paper in your bibliography.




\bibliography{refs}{}
\bibliographystyle{plain}

%\cleardoublepage \fancyhead[L]{APPENDIX}
\appendix % This line declares that you are starting the appendix.
% If you want a single Appendix and want it to be called Appendix instead of Appendix A, the following should work:
%\setcounter{secnumdepth}{-1} %This turns off automatic chapter numbering
%\chapter{Appendix}


\chapter{Appendix}

\section{Do Clicks Matter}

\subsection{Distribution of a conditioned on n}

Firstly note that:

\begin{align}
{w+a \choose w}{n \choose w+a} &= \frac{n!}{(n-w-a)!w!a} \\
	&= \frac{n!(n-a)!}{((n-a)-w)!w!a!(n-a)!} \\
	&=  {n \choose a}{n-a \choose w}
\end{align}

We may construct the probability mass function for a using the conditional distributions:

\begin{align}
p(a|p,q,n) &= \sum_C p(a|q,c)p(c|p,n) \\
 &= \sum_{c=a}^n {c \choose a} p^a(1-p)^{c-a} {n \choose c} q^c (1-q)^{n-c}
\end{align}

Try to pull out a binomial by reparameterizing with $ w = c-a$:

\begin{align}
&= \sum_{w=0}^{n-a} {w+a \choose a} p^a(1-p)^w {n \choose w+a} q^{w+a} (1-q)^{n-w+a} \\
&= {n \choose a} (pq)^a \sum_{c=a}^n {n-a \choose w} ((1-p)q)^w (1-q)^{(n-a)-w} \\
&= {n \choose a} (pq)^a ((1-p)q +  (1-q))^{n-a} \\
&= Binom(a;pq,n)
\end{align}

Ref: http://math.stackexchange.com/questions/626457/conditional-binomials

\subsection{Posterior of qp}

We have now established that a is distributed as $Bin(qp,v)$. The question remains whether the uncertainty around the parameterization qp is reduced by modelling q and p separately.

For given a,c,n counts, p and q are independent of each other. If we assume a Haldane prior $\beta(0,0)$, we therefore describe their joint density as:

\begin{align}
 f_{P,Q}(p,q|a,c,n) = Beta(p;c,n-c) Beta(q;a,c-a)
\end{align}

Using the same approach as 'Rao, Linear Statistical Inference and its Applications, 3a.3, pg 168' to get the density of pq, apply the following transformation:
\begin{align}
 u(p,q) = qp, \quad v(p,q) = q  \\
 \implies p(u,v) = \frac{u}{v}, \quad q(u,v) = v
\end{align}

\begin{align}
 f_{U,V}(u,v) &= f_{P,Q}(p(u,v),q(u,v)) 
		\frac{\partial p \partial q}{\partial u \partial v} 
		\text{ ,in range }  (u<v<1,0<u<1) \\
 &= Beta(v;c,n-c) Beta(\frac{u}{v};a,c-a) v^{-1} \\
 &= k . v^{c-1} (1-v)^{n-c-1} (\frac{u}{v})^{a-1} (\frac{v - u}{v})^{c-a-1} v^{-1} \\
 &= k . (1-v)^{n-c-1} u^{a-1} (v - u)^{c-a-1}
\end{align}

Now integrate out v. Noting that the integral has the form of a Beta function, we shift and scale the integral with a change of variable.

\begin{align}
\text{Choose }  w &= \frac{v-u}{1-u} \\
f_U(u) &= k.u^{a-1} \int_u^1 (1-v)^{n-c-1} (v - u)^{c-a-1} \mathrm{d}v \\
 &= k.u^{a-1} \int_0^1 ((1-w)(1-u))^{n-c-1} (w(1-u))^{c-a-1} (1-u) \mathrm{d}w \\
 &= k.u^{a-1} (1-u)^{n-a-1} B(n-c,c-a) \\
 &= Beta(u;a,n-a) = Beta(qp;a,n-a) 
\end{align}

Thus showing that under the Haldane prior, click count does not change our estimation. By induction this can be extended to the product of any number of beta variables with appropriate parameters.

\subsection{Posterior of qp - with priors}

We now consider the form of the posterior under different possible priors:

\begin{align}
 f_{P,Q}(p,q|a,c,n) = Beta(p;c+\alpha_p,n-c+\beta_p) Beta(q;a+\alpha_q,c-a+\beta_q)
\end{align}

We can follow the calculation through to step (44) where the $v$ terms no longer cancel out:

\begin{align}
 k . v^{\alpha_p - \alpha_q - \beta_q}(1-v)^{n-c+\beta_p-1} u^{a+\alpha_q-1} (v - u)^{c-a+\beta_q-1}
\end{align}

We see that when $ \alpha_p = \alpha_q + \beta_q $ we will get a Beta form, but otherwise it is not analytic. 

\section{Bayed Adaptive MDP}

We show the Bayes Adaptive MDP for a 2 arm, 2 round game under the 'both' model. Each state, S, is defined by the 8-tuple of prior parameters, updated according to the outcome of each trial. From each state, we have the same control options - arm1 or arm2. There are three possible state transitions - no click, click but no acquisition, click and acquisition. The transition probabilities are the expectations, given the prior values. There is a payoff of 1 for transitions with an acquisition.

\begin{align}
s0 =
 \begin{pmatrix}
  \alpha_{p,1} & \beta_{p,1} & \alpha_{q,1} & \beta_{q,1} \\
  \alpha_{p,2} & \beta_{p,2} & \alpha_{q,2} & \beta_{q,2} 
 \end{pmatrix} \\
s1 =
  \begin{pmatrix}
   \alpha_{p,1}+1 & \beta_{p,1} & \alpha_{q,1}+1 & \beta_{q,1} \\
   \alpha_{p,2} & \beta_{p,2} & \alpha_{q,2} & \beta_{q,2} 
  \end{pmatrix} \\
s2 =
  \begin{pmatrix}
   \alpha_{p,1} & \beta_{p,1}+1 & \alpha_{q,1}+1 & \beta_{q,1} \\
   \alpha_{p,2} & \beta_{p,2} & \alpha_{q,2} & \beta_{q,2} 
  \end{pmatrix} \\
\vdots \\
s6 =
  \begin{pmatrix}
   \alpha_{p,1} & \beta_{p,1} & \alpha_{q,1} & \beta_{q,1} \\
   \alpha_{p,2} & \beta_{p,2} & \alpha_{q,2} & \beta_{q,2}+1 
  \end{pmatrix}
\end{align} 

\includegraphics[scale=0.6]{BAMDP.jpg}

Note that although the posterior of pq does not have a closed form, it is still a cheap operation to take the expectation due to the independence of p and q. This means the both model is of similar order cost to the clicks-only or acquisitions-only model.

\begin{align}
	p(a=1|S,a1) = E[qp] = E[q]E[p] = 
		\left( \frac{ \alpha_{q,1} }{ \alpha_{q,1} + \beta_{q,1} } \right)
		\left( \frac{ \alpha_{p,1} }{ \alpha_{p,1} + \beta_{p,1} } \right) \\
	p(a=0,c=1|S,a1) = 
		\left( \frac{ \beta_{q,1} }{ \alpha_{q,1} + \beta_{q,1} } \right)
		\left( \frac{ \alpha_{p,1} }{ \alpha_{p,1} + \beta_{p,1} } \right) \\
	p(c=0|S,a1) = 
		\left( \frac{ \beta_{p,1} }{ \alpha_{p,1} + \beta_{p,1} } \right) 
\end{align}	

The optimal policy under this MDP is to greedily choose action with maximum value. The value for each of the 12 actions in the second round is simply the expected payoff $E[qp]$ given the state S1-S6 that the action is being taken from. The value of actions in the first round is the expectation of their immediate payoff, plus the expectation of the maximum value in the second round.

\section{Gittins index calculation}

We can adapt the above MDP to calculate a Gittins index for arm 1. To do this, we replace arm 2 with a fixed (but unknown) payoff arm. We may then calculate the maximum possible payoff of arm 2 such that we both arms have equal value in the first round. This can be achieved by backwards induction.

\end{document}
